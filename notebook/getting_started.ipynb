{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FjHFCETf3KUq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FjHFCETf3KUq",
    "outputId": "f4557dc9-eebe-4668-bf10-687ebb5731d5"
   },
   "outputs": [],
   "source": [
    "# Install system dependencies\n",
    "!apt-get update && apt-get install -y \\\n",
    "    python3-dev \\\n",
    "    swig \\\n",
    "    python3-pygame \\\n",
    "    libsdl2-dev \\\n",
    "    libjpeg-dev \\\n",
    "    zlib1g-dev\n",
    "\n",
    "# Install Python packages\n",
    "!pip install \"pettingzoo[atari]==1.24.3\" gymnasium[atari] numpy pygame\n",
    "\n",
    "# Install and setup AutoROM\n",
    "!pip install autorom\n",
    "!AutoROM --accept-license"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9928b304-95fd-4ad6-9ada-1ec6b94b1998",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9928b304-95fd-4ad6-9ada-1ec6b94b1998",
    "outputId": "87bd91a3-214f-45db-94df-bc266fdc5107"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/ml-arena/pong2024.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437b5c0c-38ee-4e17-8de5-efe87df14b39",
   "metadata": {
    "id": "437b5c0c-38ee-4e17-8de5-efe87df14b39"
   },
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af0c094-4e5c-42b1-adb2-f6aff20e0196",
   "metadata": {
    "id": "8af0c094-4e5c-42b1-adb2-f6aff20e0196"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pettingzoo.atari import pong_v3\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a468e0-c970-4762-b0d6-5a201728db9d",
   "metadata": {
    "id": "48a468e0-c970-4762-b0d6-5a201728db9d"
   },
   "source": [
    "### 1. Environment Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71628b16-9a65-4449-b95d-6f955f7a7eaf",
   "metadata": {
    "id": "71628b16-9a65-4449-b95d-6f955f7a7eaf"
   },
   "source": [
    "### Environment Creation and Reset\n",
    "```python\n",
    "env = pong_v3.env()  # Create the environment\n",
    "env.reset()          # Reset the environment to initial state\n",
    "```\n",
    "\n",
    "### Key Properties\n",
    "- `env.agents`: List of active agents in the environment\n",
    "- `env.action_space(agent)`: Action space for specific agent\n",
    "- `env.observation_space(agent)`: Observation space for specific agent\n",
    "\n",
    "### Environment Interaction Methods\n",
    "\n",
    "#### env.last()\n",
    "Returns tuple of `(observation, reward, terminated, truncated, info)`\n",
    "- `observation`: NumPy array (210, 160, 3) representing game state\n",
    "- `reward`: Float value indicating reward from last action\n",
    "- `terminated`: Boolean indicating if episode ended naturally\n",
    "- `truncated`: Boolean indicating if episode was artificially terminated\n",
    "- `info`: Dictionary with additional information\n",
    "\n",
    "#### env.step(action)\n",
    "- Takes an action for the current agent\n",
    "- Actions must be valid for the current agent's action space\n",
    "- Automatically handles agent cycling\n",
    "\n",
    "#### env.agent_iter()\n",
    "- Iterator that cycles through active agents\n",
    "- Typically used in the main game loop\n",
    "- Returns the current agent's name\n",
    "\n",
    "### Environment Management\n",
    "```python\n",
    "env.close()  # Clean up environment resources\n",
    "```\n",
    "\n",
    "## Observation Space Details\n",
    "- Shape: (210, 160, 3)\n",
    "  - Height: 210 pixels\n",
    "  - Width: 160 pixels\n",
    "  - Channels: 3 (RGB)\n",
    "- Values: 0-255 (uint8)\n",
    "- Each observation is a complete frame of the game\n",
    "\n",
    "## Action Space Details\n",
    "- Type: Discrete(6)\n",
    "- Actions:\n",
    "  - 0: No operation\n",
    "  - 1: Fire\n",
    "  - 2: Move right\n",
    "  - 3: Move left\n",
    "  - 4: Fire right\n",
    "  - 5: Fire left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d16a04-31c5-4ad3-abca-1165681baf68",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "74d16a04-31c5-4ad3-abca-1165681baf68",
    "outputId": "c05cbbf5-1f7e-48dd-fff7-3e610edf0eae"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pettingzoo.atari import pong_v3\n",
    "\n",
    "# Create and initialize the environment\n",
    "print(\"Creating Pong Environment...\")\n",
    "env = pong_v3.env()\n",
    "env.reset()\n",
    "\n",
    "# Display environment information\n",
    "print(\"\\nEnvironment Information:\")\n",
    "print(f\"Agents: {env.agents}\")\n",
    "print(f\"Action Space: {env.action_space('first_0')}\")\n",
    "print(f\"Observation Space: {env.observation_space('first_0')}\")\n",
    "\n",
    "# Get the initial observation for the first agent\n",
    "observation, reward, terminated, truncated, info = env.last()\n",
    "agent = env.agents[0]  # Get the first agent\n",
    "\n",
    "print(f\"\\nObservation Details for {agent}:\")\n",
    "print(f\"Shape: {observation.shape}\")\n",
    "print(f\"Value range: [{observation.min()}, {observation.max()}]\")\n",
    "print(f\"Reward: {reward}\")\n",
    "print(f\"Game Status - Terminated: {terminated}, Truncated: {truncated}\")\n",
    "print(f\"Additional Info: {info}\")\n",
    "\n",
    "# Visualize the observation\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(observation)\n",
    "plt.title(f'Game State Observation for {agent}')\n",
    "plt.axis('on')\n",
    "plt.colorbar(label='Pixel Values')\n",
    "plt.show()\n",
    "\n",
    "# Show RGB channels separately\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "fig.suptitle(f'RGB Channel Breakdown of Game State')\n",
    "\n",
    "channels = ['Red', 'Green', 'Blue']\n",
    "for i, (ax, channel) in enumerate(zip(axes, channels)):\n",
    "    ax.imshow(observation[:, :, i], cmap='gray')\n",
    "    ax.set_title(f'{channel} Channel')\n",
    "    ax.axis('on')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f51c5b-23de-4608-9ac5-6981d42e9e58",
   "metadata": {
    "id": "55f51c5b-23de-4608-9ac5-6981d42e9e58"
   },
   "source": [
    "### 2. Agent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c84dd2f-d290-4f42-8184-b684255b28d5",
   "metadata": {
    "id": "4c84dd2f-d290-4f42-8184-b684255b28d5"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"Base Agent class for Pong competition.\"\"\"\n",
    "    def __init__(self, env, player_name=None):\n",
    "        self.env = env\n",
    "        self.player_name = player_name\n",
    "\n",
    "    def choose_action(self, observation, reward=0.0, terminated=False, truncated=False, info=None):\n",
    "        \"\"\"Choose an action based on the current game state.\"\"\"\n",
    "        return self.env.action_space(self.player_name).sample()\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"Learning method - to be implemented by specific agents.\"\"\"\n",
    "        pass\n",
    "\n",
    "class AgentAlwaysLeft(Agent):\n",
    "    \"\"\"Agent that always moves left.\"\"\"\n",
    "    def choose_action(self, observation, reward=0.0, terminated=False, truncated=False, info=None):\n",
    "        \"\"\"Always choose the 'move left' action (3).\"\"\"\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60a2688-c590-4dcf-a287-f8ff37a845b4",
   "metadata": {
    "id": "b60a2688-c590-4dcf-a287-f8ff37a845b4"
   },
   "source": [
    "### 3. Running a Simple Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4c456b-1dd2-4fd1-a918-effdc126510c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cc4c456b-1dd2-4fd1-a918-effdc126510c",
    "outputId": "df4231d9-3750-4277-f36d-b5c3b6f6cae1"
   },
   "outputs": [],
   "source": [
    "def run_match(env, agent1, agent2, max_cycles=500):\n",
    "    \"\"\"\n",
    "    Run a match between two agents and return their cumulative rewards.\n",
    "\n",
    "    Args:\n",
    "        env: The game environment\n",
    "        agent1: First agent\n",
    "        agent2: Second agent\n",
    "        max_cycles: Maximum number of cycles before forcing end of game\n",
    "\n",
    "    Returns:\n",
    "        tuple: (agent1_reward, agent2_reward) - Cumulative rewards for both agents\n",
    "    \"\"\"\n",
    "    env.reset()\n",
    "\n",
    "    # Assign player names to agents\n",
    "    agent1.player_name = env.agents[0]\n",
    "    agent2.player_name = env.agents[1]\n",
    "\n",
    "    # Initialize reward tracking\n",
    "    cumulative_rewards = {\n",
    "        agent1.player_name: 0,\n",
    "        agent2.player_name: 0\n",
    "    }\n",
    "    nb_step = 0\n",
    "\n",
    "    for agent in env.agent_iter():\n",
    "        observation, reward, terminated, truncated, info = env.last()\n",
    "\n",
    "        # Update cumulative rewards\n",
    "        cumulative_rewards[agent] += reward\n",
    "\n",
    "        if terminated or truncated:\n",
    "            action = None\n",
    "        else:\n",
    "            # Choose action based on which agent's turn it is\n",
    "            if agent == agent1.player_name:\n",
    "                action = agent1.choose_action(observation, reward, terminated, truncated, info)\n",
    "            else:\n",
    "                action = agent2.choose_action(observation, reward, terminated, truncated, info)\n",
    "\n",
    "        env.step(action)\n",
    "        nb_step += 1\n",
    "\n",
    "        if terminated or truncated or nb_step > max_cycles:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    return cumulative_rewards[agent1.player_name], cumulative_rewards[agent2.player_name]\n",
    "\n",
    "# Create environment and agents\n",
    "env = pong_v3.env()\n",
    "random_agent = Agent(env)\n",
    "always_left_agent = AgentAlwaysLeft(env)\n",
    "\n",
    "print(\"\\nRunning match: Random Agent vs Always Left Agent\")\n",
    "agent1_reward, agent2_reward = run_match(env, random_agent, always_left_agent)\n",
    "print(f\"Random Agent reward: {agent1_reward}\")\n",
    "print(f\"Always Left Agent reward: {agent2_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda80f9d-c093-47e1-8c33-016b3f6cbbdd",
   "metadata": {
    "id": "bda80f9d-c093-47e1-8c33-016b3f6cbbdd"
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12eb144-7b6b-494d-934a-506d83d87d6f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d12eb144-7b6b-494d-934a-506d83d87d6f",
    "outputId": "89241db4-56c0-40e4-d97d-d6009148e2ac"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pettingzoo.atari import pong_v3\n",
    "from typing import Dict, List, Tuple, Type\n",
    "import time\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Import evaluation function\n",
    "from pong2024.eval.evaluate_against_multiple_agents import evaluate_against_multiple_agents, visualize_multiple_matchups\n",
    "\n",
    "\n",
    "# Define example agents for testing\n",
    "class YourAgent:\n",
    "    \"\"\"Example of a custom agent - currently just random actions.\"\"\"\n",
    "    def __init__(self, env, player_name=None):\n",
    "        self.env = env\n",
    "        self.player_name = player_name\n",
    "\n",
    "    def choose_action(self, observation, reward=0.0, terminated=False, truncated=False, info=None):\n",
    "        \"\"\"Choose action randomly.\"\"\"\n",
    "        return self.env.action_space(self.player_name).sample()\n",
    "\n",
    "class AlwaysLeftAgent:\n",
    "    \"\"\"Agent that always moves left.\"\"\"\n",
    "    def __init__(self, env, player_name=None):\n",
    "        self.env = env\n",
    "        self.player_name = player_name\n",
    "\n",
    "    def choose_action(self, observation, reward=0.0, terminated=False, truncated=False, info=None):\n",
    "        \"\"\"Always choose move left action (3).\"\"\"\n",
    "        return 3\n",
    "\n",
    "class AlwaysRightAgent:\n",
    "    \"\"\"Agent that always moves right.\"\"\"\n",
    "    def __init__(self, env, player_name=None):\n",
    "        self.env = env\n",
    "        self.player_name = player_name\n",
    "\n",
    "    def choose_action(self, observation, reward=0.0, terminated=False, truncated=False, info=None):\n",
    "        \"\"\"Always choose move right action (2).\"\"\"\n",
    "        return 2\n",
    "\n",
    "# Create environment\n",
    "env = pong_v3.env()\n",
    "\n",
    "# Define list of opponent agents to evaluate against\n",
    "opponent_agents = [\n",
    "    AlwaysLeftAgent,   # Deterministic opponent - always moves left\n",
    "    AlwaysRightAgent,  # Deterministic opponent - always moves right\n",
    "    YourAgent          # Random opponent\n",
    "]\n",
    "\n",
    "print(\"Starting evaluation against multiple opponents...\")\n",
    "\n",
    "# Run evaluation against all opponents\n",
    "results = evaluate_against_multiple_agents(\n",
    "    env=env,\n",
    "    main_agent_class=YourAgent,        # Your agent to evaluate\n",
    "    opponent_classes=opponent_agents,   # List of opponents\n",
    "    n_games_per_matchup=10,            # Number of games per opponent\n",
    "    max_cycles=100,                  # Maximum steps per game\n",
    "    seed=42,                            # For reproducibility\n",
    "    n_verbose_game=1\n",
    ")\n",
    "\n",
    "# Print summary results\n",
    "print(\"\\nEvaluation Summary:\")\n",
    "print(f\"Overall win rate: {results['summary']['main_agent_overall_winrate']:.1%}\")\n",
    "print(f\"Average score: {results['summary']['main_agent_average_score']:.1f}\")\n",
    "\n",
    "print(\"\\nPerformance against each opponent:\")\n",
    "for matchup in results['matchups']:\n",
    "    print(f\"\\nVs {matchup['opponent_class']}:\")\n",
    "    print(f\"Win rate: {matchup['main_agent_winrate']:.1%}\")\n",
    "    print(f\"Average score: {matchup['main_agent_avg_score']:.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viikjbEPoKPy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 996
    },
    "id": "viikjbEPoKPy",
    "outputId": "59f7540c-b733-41fa-c459-c1f452fc171b"
   },
   "outputs": [],
   "source": [
    "\n",
    "visualize_multiple_matchups(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14a56af-ec52-41f7-80ef-39ad2f8521db",
   "metadata": {
    "id": "a14a56af-ec52-41f7-80ef-39ad2f8521db"
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266da67e-342c-4cb0-b4f9-7a0bc927a3c3",
   "metadata": {
    "id": "266da67e-342c-4cb0-b4f9-7a0bc927a3c3"
   },
   "source": [
    "The raw observation from the Pong environment (210, 160, 3) is quite large and complex,\n",
    "containing a lot of unnecessary information. We can make learning easier by:\n",
    "\n",
    "1. Dimension Reduction:\n",
    "   - Extract play area (removing score/info areas)\n",
    "   - Convert to grayscale (remove color channels)\n",
    "   - Resize to smaller dimensions\n",
    "\n",
    "2. Player Perspective Normalization:\n",
    "   - The game looks different for player 1 vs player 2\n",
    "   - Flipping the image for player 2 makes both perspectives similar (but action should be reverse)\n",
    "   - This helps the agent learn a single strategy for both sides\n",
    "\n",
    "3. Image Processing:\n",
    "   - Gaussian smoothing reduces noise\n",
    "   - Binary thresholding separates objects clearly\n",
    "   - Normalization scales values to [0,1]\n",
    "\n",
    "4. Temporal Features:\n",
    "   - Pong is a dynamic environment\n",
    "   - Consider using frame differences to capture motion\n",
    "   - Stack multiple frames to provide temporal context\n",
    "   - Apply same preprocessing to difference frames\n",
    "5. Extract more specific information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1WO50BB6siNz",
   "metadata": {
    "id": "1WO50BB6siNz"
   },
   "outputs": [],
   "source": [
    "from pong2024.feature_engineering.feature_engineering import simplified_preprocess_image\n",
    "\n",
    "def visualize_preprocessing_steps(observation):\n",
    "    \"\"\"\n",
    "    Visualize preprocessing steps: Original -> Play Area -> Grayscale -> Binary\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(16, 4))\n",
    "\n",
    "    # Original\n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.imshow(observation)\n",
    "    plt.title('Original')\n",
    "    plt.axis('on')\n",
    "\n",
    "    # Play area\n",
    "    y_bottom, y_top = 34, 194\n",
    "    play_area = observation[y_bottom:y_top, :, :]\n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.imshow(play_area)\n",
    "    plt.title('Play Area')\n",
    "    plt.axis('on')\n",
    "\n",
    "    # Grayscale\n",
    "    gray = cv2.cvtColor(play_area, cv2.COLOR_RGB2GRAY)\n",
    "    plt.subplot(1, 4, 3)\n",
    "    plt.imshow(gray, cmap='gray')\n",
    "    plt.title('Grayscale')\n",
    "    plt.axis('on')\n",
    "\n",
    "    # Binary (0,1)\n",
    "    processed = simplified_preprocess_image(observation)\n",
    "    plt.subplot(1, 4, 4)\n",
    "    plt.imshow(processed, cmap='binary')\n",
    "    plt.title('Binary')\n",
    "    plt.axis('on')\n",
    "\n",
    "    # Verify binary values\n",
    "    unique_values = np.unique(processed)\n",
    "    print(f\"Unique values in binary output: {unique_values}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rcHyIsF-uBS9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 863
    },
    "id": "rcHyIsF-uBS9",
    "outputId": "39ce5eaf-f3cc-4339-de3d-fa0fe1841687"
   },
   "outputs": [],
   "source": [
    "steps = [50,150,222]\n",
    "# Collect multiple observations\n",
    "observations = collect_game_observations(steps=steps)\n",
    "\n",
    "# Visualize preprocessing steps for each observation\n",
    "for i, obs in enumerate(observations):\n",
    "    print(f\"\\nObservation {steps[i]}:\")\n",
    "    gray = visualize_preprocessing_steps(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec27181-19c1-4df8-8007-a421bf5c1984",
   "metadata": {
    "id": "3ec27181-19c1-4df8-8007-a421bf5c1984"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Wr2muobO4cGw",
   "metadata": {
    "id": "Wr2muobO4cGw"
   },
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    \"\"\"Base Agent class for Pong competition.\"\"\"\n",
    "    def __init__(self, env, player_name=None):\n",
    "        self.env = env\n",
    "        self.player_name = player_name\n",
    "\n",
    "    def choose_action(self, observation, reward=0.0, terminated=False, truncated=False, info=None):\n",
    "        \"\"\"Choose an action based on the current game state.\"\"\"\n",
    "        return self.env.action_space(self.player_name).sample()\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"Learning method - to be implemented by specific agents.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca6f946-6d35-4d93-b2c0-a5adda942de6",
   "metadata": {
    "id": "aca6f946-6d35-4d93-b2c0-a5adda942de6"
   },
   "outputs": [],
   "source": [
    "  # ML-Arena: Pong 2024 - Q-Learning Agent Training\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "import os\n",
    "from pettingzoo.atari import pong_v3\n",
    "from pong2024.train.train_sequential import train_sequential\n",
    "from pong2024.feature_engineering.feature_engineering import simplified_preprocess_image\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Convolutional Q-Network optimized for 28x28 Pong binary input with 2 channels\"\"\"\n",
    "    def __init__(self, n_actions=6):\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        # Modified architecture for 2-channel input (current and previous frames)\n",
    "        self.conv = nn.Sequential(\n",
    "            # Input: 2x28x28 (2 channels for current and previous frames)\n",
    "            nn.Conv2d(2, 16, kernel_size=3, stride=1, padding=1),  # Output: 16x28x28\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # Output: 16x14x14\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),  # Output: 32x14x14\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # Output: 32x7x7\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),  # Output: 32x7x7\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(32 * 7 * 7, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()  # x should already be [batch_size, 2, 28, 28]\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        return self.fc(x)\n",
    "\n",
    "class QAgent:\n",
    "    \"\"\"Q-Learning Agent with 2-Channel Input, Experience Replay, and Action Flipping\"\"\"\n",
    "    def __init__(self, env, player_name=None, auto_load_path=None,\n",
    "                 auto_save_n_steps=100, auto_save_suffix=\"q_agent.pth\"):\n",
    "        self.env = env\n",
    "        self.player_name = player_name\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Save/Load parameters\n",
    "        self.auto_save_n_steps = auto_save_n_steps\n",
    "        self.auto_save_suffix = auto_save_suffix\n",
    "\n",
    "        # Q-Network and target network\n",
    "        self.q_network = QNetwork().to(self.device)\n",
    "        self.target_network = QNetwork().to(self.device)\n",
    "\n",
    "        # Load pretrained model if available\n",
    "        if auto_load_path and os.path.exists(auto_load_path):\n",
    "            print(f\"Loading model from {auto_load_path}\")\n",
    "            self.load(auto_load_path)\n",
    "        else:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.0001)\n",
    "        self.memory = deque(maxlen=10000)\n",
    "\n",
    "        # Training parameters\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.target_update = 1000\n",
    "        self.steps = 0\n",
    "\n",
    "        # Store previous observation\n",
    "        self.previous_observation = None\n",
    "\n",
    "        # Action mapping for second player\n",
    "        self.action_mapping = {\n",
    "            0: 0,  # NOOP -> NOOP\n",
    "            1: 1,  # FIRE -> FIRE\n",
    "            2: 3,  # RIGHT -> LEFT\n",
    "            3: 2,  # LEFT -> RIGHT\n",
    "            4: 5,  # RIGHTFIRE -> LEFTFIRE\n",
    "            5: 4   # LEFTFIRE -> RIGHTFIRE\n",
    "        }\n",
    "\n",
    "    def preprocess_frames(self, current_observation, flip=False):\n",
    "        \"\"\"Preprocess current frame and combine with previous frame\"\"\"\n",
    "        # Preprocess current frame\n",
    "        current_frame = simplified_preprocess_image(current_observation, flip=flip)\n",
    "\n",
    "        # If no previous observation, use zeros\n",
    "        if self.previous_observation is None:\n",
    "            previous_frame = np.zeros_like(current_frame)\n",
    "        else:\n",
    "            previous_frame = simplified_preprocess_image(self.previous_observation, flip=flip)\n",
    "\n",
    "        # Stack frames into 2 channels\n",
    "        stacked_frames = np.stack([current_frame, previous_frame])\n",
    "\n",
    "        # Update previous observation\n",
    "        self.previous_observation = current_observation.copy()\n",
    "\n",
    "        return stacked_frames\n",
    "\n",
    "    def choose_action(self, observation, reward=0.0, terminated=False, truncated=False, info=None):\n",
    "        \"\"\"Choose action using epsilon-greedy policy with 2-channel input\"\"\"\n",
    "        is_second_player = (self.player_name == \"second_0\")\n",
    "\n",
    "        # Preprocess and stack frames\n",
    "        state = self.preprocess_frames(observation, flip=is_second_player)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "\n",
    "        if random.random() < self.epsilon:\n",
    "            action = self.env.action_space(self.player_name).sample()\n",
    "            return self.flip_action(action) if is_second_player else action\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_network(state)\n",
    "            action = q_values.argmax().item()\n",
    "            return self.flip_action(action) if is_second_player else action\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store transition in replay memory\"\"\"\n",
    "        # If second player, store the flipped action\n",
    "        if self.player_name == \"second_0\":\n",
    "            action = self.flip_action(action)\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def flip_action(self, action):\n",
    "        \"\"\"Map actions for second player\"\"\"\n",
    "        if self.player_name == \"second_0\":\n",
    "            return self.action_mapping[action]\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"Train on a batch from replay memory\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        state_batch = torch.FloatTensor(np.array([t[0] for t in batch])).to(self.device)\n",
    "        action_batch = torch.LongTensor(np.array([t[1] for t in batch])).to(self.device)\n",
    "        reward_batch = torch.FloatTensor(np.array([t[2] for t in batch])).to(self.device)\n",
    "        next_state_batch = torch.FloatTensor(np.array([t[3] for t in batch])).to(self.device)\n",
    "        done_batch = torch.FloatTensor(np.array([t[4] for t in batch])).to(self.device)\n",
    "\n",
    "        # Compute current Q values\n",
    "        current_q_values = self.q_network(state_batch).gather(1, action_batch.unsqueeze(1))\n",
    "\n",
    "        # Compute target Q values\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_state_batch).max(1)[0]\n",
    "            target_q_values = reward_batch + (1 - done_batch) * self.gamma * next_q_values\n",
    "\n",
    "        # Compute loss and update\n",
    "        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update target network and handle auto-save\n",
    "        self.steps += 1\n",
    "        if self.steps % self.target_update == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "        if self.auto_save_n_steps > 0 and self.steps % self.auto_save_n_steps == 0:\n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "            save_path = f\"{timestamp}_{self.auto_save_suffix}\"\n",
    "            self.save(save_path)\n",
    "            print(f\"Auto-saved model to {save_path}\")\n",
    "\n",
    "        # Decay epsilon\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"Save model state\"\"\"\n",
    "        torch.save({\n",
    "            'q_network_state_dict': self.q_network.state_dict(),\n",
    "            'target_network_state_dict': self.target_network.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'epsilon': self.epsilon,\n",
    "            'steps': self.steps\n",
    "        }, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        \"\"\"Load model state\"\"\"\n",
    "        checkpoint = torch.load(path)\n",
    "        self.q_network.load_state_dict(checkpoint['q_network_state_dict'])\n",
    "        self.target_network.load_state_dict(checkpoint['target_network_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.epsilon = checkpoint['epsilon']\n",
    "        self.steps = checkpoint['steps']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nXibvV3L3M_n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "nXibvV3L3M_n",
    "outputId": "55b070eb-cca4-458e-d5f7-51e73a1be281"
   },
   "outputs": [],
   "source": [
    "# Training setup\n",
    "def make_env():\n",
    "    return pong_v3.env()\n",
    "\n",
    "# Define opponents with their probabilities\n",
    "opponent_classes = [RandomAgent, AlwaysLeftAgent]\n",
    "opponent_probs = [0.9, 0.1]  # 90% Random, 10% AlwaysLeft\n",
    "\n",
    "# Train the agent\n",
    "print(\"Starting training...\")\n",
    "results = train_sequential(\n",
    "    make_env=make_env,\n",
    "    main_agent_class=QAgent,\n",
    "    opponent_classes=opponent_classes,\n",
    "    opponent_probs=opponent_probs,\n",
    "    n_total_episodes=10000,  # Total training episodes\n",
    "    eval_frequency=10,      # Evaluate every 100 episodes\n",
    "    max_cycles=10000         # Max steps per episode\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hs4asmjiMSnL",
   "metadata": {
    "id": "Hs4asmjiMSnL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
